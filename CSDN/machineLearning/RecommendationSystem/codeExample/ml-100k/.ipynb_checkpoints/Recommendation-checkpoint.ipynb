{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#用户数目和电影数目\n",
    "from utils import n_Users, n_Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## SVD,基于模型的协同过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division  \n",
    "import numpy as np  \n",
    "import scipy as sp  \n",
    "from numpy.random import random  \n",
    "\n",
    "class  SVD_C:  \n",
    "    def __init__(self, X, k=20):  \n",
    "        ''''' \n",
    "            k  is the number of latent componets \n",
    "        '''  \n",
    "        self.X = X  \n",
    "        self.k = k  \n",
    "        self.mu = np.mean(self.R)  #平均打分\n",
    "        \n",
    "        #模型参数初始化\n",
    "        self.bi={}  \n",
    "        self.bu={}  \n",
    "        \n",
    "        self.qi={}  \n",
    "        self.pu={}  \n",
    "        \n",
    "        self.ItemsForUser={}  #每个Item对应的用户\n",
    "        self.UsersForItem={}  #每个用户对哪些Item打过分\n",
    "        \n",
    "        for i in range(self.X.shape[0]):  \n",
    "            uid=self.X[i][0]  \n",
    "            i_id=self.X[i][1]  \n",
    "            rat=self.X[i][2] \n",
    "            \n",
    "            self.ItemsForUser.setdefault(i_id,{})  \n",
    "            self.UsersForItem.setdefault(uid,{}) \n",
    "            \n",
    "            self.ItemsForUser[i_id][uid]=rat  \n",
    "            self.UsersForItem[uid][i_id]=rat  \n",
    "            \n",
    "            self.bi.setdefault(i_id,0)  \n",
    "            self.bu.setdefault(uid,0)  \n",
    "            \n",
    "            self.qi.setdefault(i_id,random((self.k,1))/10*(np.sqrt(self.k)))  \n",
    "            self.pu.setdefault(uid,random((self.k,1))/10*(np.sqrt(self.k)))  \n",
    "                    \n",
    "    #根据当前参数，预测用户uid对Item（i_id）的打分\n",
    "    def pred(self,uid,i_id):  \n",
    "        self.bi.setdefault(i_id,0)  \n",
    "        self.bu.setdefault(uid,0)  \n",
    "        \n",
    "        self.qi.setdefault(i_id,np.zeros((self.k,1)))  \n",
    "        self.pu.setdefault(uid,np.zeros((self.k,1)))  \n",
    "        \n",
    "        if (self.qi[i_id]==None):  \n",
    "            self.qi[i_id]=np.zeros((self.k,1))  \n",
    "        if (self.pu[uid]==None):  \n",
    "            self.pu[uid]=np.zeros((self.k,1))  \n",
    "        \n",
    "        ans=self.mu + self.bi[i_id] + self.bu[uid] + np.sum(self.qi[i_id]*self.pu[uid])  \n",
    "        \n",
    "        #将打分范围控制在1-5之间\n",
    "        if ans>5:  \n",
    "            return 5  \n",
    "        elif ans<1:  \n",
    "            return 1  \n",
    "        return ans  \n",
    "    \n",
    "    #gamma：为学习率\n",
    "    #Lambda：正则参数\n",
    "    def train(self,steps=20,gamma=0.04,Lambda=0.15):  \n",
    "        for step in range(steps):  \n",
    "            print 'the ',step,'-th  step is running'  \n",
    "            rmse_sum=0.0 \n",
    "            \n",
    "            #将训练样本打散顺序\n",
    "            kk = np.random.permutation(self.X.shape[0])  \n",
    "            for j in range(self.X.shape[0]):  \n",
    "                \n",
    "                #每次一个训练样本\n",
    "                i=kk[j]  \n",
    "                uid=self.X[i][0]  \n",
    "                i_id=self.X[i][1]  \n",
    "                rat=self.X[i][2]  \n",
    "                \n",
    "                #预测残差\n",
    "                eui=rat-self.pred(uid,i_id)  \n",
    "                #残差平方和\n",
    "                rmse_sum+=eui**2  \n",
    "                \n",
    "                #随机梯度下降，更新\n",
    "                self.bu[uid]+=gamma*(eui-Lambda*self.bu[uid])  \n",
    "                self.bi[i_id]+=gamma*(eui-Lambda*self.bi[i_id]) \n",
    "                \n",
    "                temp=self.qi[i_id]  \n",
    "                self.qi[i_id]+=gamma*(eui*self.pu[uid]-Lambda*self.qi[i_id])  \n",
    "                self.pu[uid]+=gamma*(eui*temp-Lambda*self.pu[uid])  \n",
    "            \n",
    "            #学习率递减\n",
    "            gamma=gamma*0.93  \n",
    "            print \"the rmse of this step on train data is \",np.sqrt(rmse_sum/self.X.shape[0])  \n",
    "            #self.test(test_data)  \n",
    "            \n",
    "    def test(self,test_X):  \n",
    "        output=[]  \n",
    "        sums=0  \n",
    "        test_X=np.array(test_X)  \n",
    "          \n",
    "        for i in range(test_X.shape[0]):  \n",
    "            pre=self.pred(test_X[i][0],test_X[i][1])  \n",
    "            output.append(pre)  \n",
    "            sums+=(pre-test_X[i][2])**2  \n",
    "        rmse=np.sqrt(sums/test_X.shape[0])  \n",
    "        print \"the rmse on test data is \",rmse  \n",
    "        return output  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成训练数据...\n",
      "\n",
      "train.csv:500 (userId, eventId)=(123290209, 1887085024)\n",
      "train.csv:1000 (userId, eventId)=(272886293, 199858305)\n",
      "train.csv:1500 (userId, eventId)=(395305791, 1582270949)\n",
      "train.csv:2000 (userId, eventId)=(527523423, 3272728211)\n",
      "train.csv:2500 (userId, eventId)=(651258472, 792632006)\n",
      "train.csv:3000 (userId, eventId)=(811791433, 524756826)\n",
      "train.csv:3500 (userId, eventId)=(985547042, 1269035551)\n",
      "train.csv:4000 (userId, eventId)=(1107615001, 173949238)\n",
      "train.csv:4500 (userId, eventId)=(1236336671, 3849306291)\n",
      "train.csv:5000 (userId, eventId)=(1414301782, 2652356640)\n",
      "train.csv:5500 (userId, eventId)=(1595465532, 955398943)\n",
      "train.csv:6000 (userId, eventId)=(1747091728, 2131379889)\n",
      "train.csv:6500 (userId, eventId)=(1914182220, 955398943)\n",
      "train.csv:7000 (userId, eventId)=(2071842684, 1076364848)\n",
      "train.csv:7500 (userId, eventId)=(2217853337, 3051438735)\n",
      "train.csv:8000 (userId, eventId)=(2338481531, 2525447278)\n",
      "train.csv:8500 (userId, eventId)=(2489551967, 520657921)\n",
      "train.csv:9000 (userId, eventId)=(2650493630, 87962584)\n",
      "train.csv:9500 (userId, eventId)=(2791418962, 4223848259)\n",
      "train.csv:10000 (userId, eventId)=(2903662804, 2791462807)\n",
      "train.csv:10500 (userId, eventId)=(3036141956, 3929507420)\n",
      "train.csv:11000 (userId, eventId)=(3176074542, 3459485614)\n",
      "train.csv:11500 (userId, eventId)=(3285425249, 2271782630)\n",
      "train.csv:12000 (userId, eventId)=(3410667855, 1063772489)\n",
      "train.csv:12500 (userId, eventId)=(3531604778, 2584839423)\n",
      "train.csv:13000 (userId, eventId)=(3686871863, 53495098)\n",
      "train.csv:13500 (userId, eventId)=(3833637800, 2415873572)\n",
      "train.csv:14000 (userId, eventId)=(3944021305, 2096772901)\n",
      "train.csv:14500 (userId, eventId)=(4075466480, 3567240505)\n",
      "train.csv:15000 (userId, eventId)=(4197193550, 1628057176)\n",
      "生成预测数据...\n",
      "\n",
      "test.csv:500 (userId, eventId)=(182290053, 2529072432)\n",
      "test.csv:1000 (userId, eventId)=(433510318, 4244463632)\n",
      "test.csv:1500 (userId, eventId)=(632808865, 2845303452)\n",
      "test.csv:2000 (userId, eventId)=(813611885, 2036538169)\n",
      "test.csv:2500 (userId, eventId)=(1010701404, 303459881)\n",
      "test.csv:3000 (userId, eventId)=(1210932037, 2529072432)\n",
      "test.csv:3500 (userId, eventId)=(1452921099, 2705317682)\n",
      "test.csv:4000 (userId, eventId)=(1623287180, 1626678328)\n",
      "test.csv:4500 (userId, eventId)=(1855201342, 2603032829)\n",
      "test.csv:5000 (userId, eventId)=(2083900381, 2529072432)\n",
      "test.csv:5500 (userId, eventId)=(2318415276, 2509151803)\n",
      "test.csv:6000 (userId, eventId)=(2528161539, 4025975316)\n",
      "test.csv:6500 (userId, eventId)=(2749110768, 4244406355)\n",
      "test.csv:7000 (userId, eventId)=(2927772127, 1532377761)\n",
      "test.csv:7500 (userId, eventId)=(3199685636, 1776393554)\n",
      "test.csv:8000 (userId, eventId)=(3393388475, 680270887)\n",
      "test.csv:8500 (userId, eventId)=(3601169721, 154434302)\n",
      "test.csv:9000 (userId, eventId)=(3828963415, 3067222491)\n",
      "test.csv:9500 (userId, eventId)=(4018723397, 2522610844)\n",
      "test.csv:10000 (userId, eventId)=(4180064266, 2658555390)\n"
     ]
    }
   ],
   "source": [
    "# 这是构建特征部分\n",
    "from __future__ import division\n",
    "\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "class DataRewriter:\n",
    "  def __init__(self):\n",
    "    # 读入数据做初始化\n",
    "    \n",
    "    # 评分矩阵\n",
    "    self.userMovieScores = sio.mmread(\"PE_userMovieScores\").todense()\n",
    "    \n",
    "    #对电影打过分的用户\n",
    "    usersForMovie = cPickle.load(open(\"PE_usersForMoive.pkl\", 'rb'))\n",
    "    #用户打过分的电影\n",
    "    moviesForUser = cPickle.load(open(\"PE_moviesForUser.pkl\", 'rb'))\n",
    "    \n",
    "    # 根据用户属性计算的用户相似度矩阵\n",
    "    self.userSimMatrix = sio.mmread(\"US_userSimMatrix\").todense()\n",
    "    \n",
    "    # 根据电影属性计算的电影相似度矩阵，暂时没有\n",
    "    #self.moviePropSim = sio.mmread(\"EV_eventPropSim\").todense()\n",
    "    \n",
    "  def userReco(self, userId, movieId):\n",
    "    \"\"\"\n",
    "    根据User-based协同过滤，得到movie的推荐度\n",
    "    基本的伪代码思路如下：\n",
    "    for item i\n",
    "      for every other user v that has a preference for i\n",
    "        compute similarity s between u and v\n",
    "        incorporate v's preference for i weighted by s into running aversge\n",
    "    return top items ranked by weighted average\n",
    "    \"\"\"\n",
    "    i = self.userIndex[userId]\n",
    "    j = self.movietIndex[eventId]\n",
    "    vs = self.userMovieScores[:, j]\n",
    "    sims = self.userSimMatrix[i, :]\n",
    "    prod = sims * vs\n",
    "    try:\n",
    "      return prod[0, 0] - self.userEventScores[i, j]\n",
    "    except IndexError:\n",
    "      return 0\n",
    "\n",
    "  def eventReco(self, userId, eventId):\n",
    "    \"\"\"\n",
    "    根据基于物品的协同过滤，得到Event的推荐度\n",
    "    基本的伪代码思路如下：\n",
    "    for item i \n",
    "      for every item j tht u has a preference for\n",
    "        compute similarity s between i and j\n",
    "        add u's preference for j weighted by s to a running average\n",
    "    return top items, ranked by weighted average\n",
    "    \"\"\"\n",
    "    \n",
    "    self.similarity.setdefault(m1,{})  \n",
    "    self.similarity.setdefault(m2,{})  \n",
    "        \n",
    "    self.movie_user.setdefault(m1,{})  \n",
    "    self.movie_user.setdefault(m2,{})  \n",
    "    self.similarity[m1].setdefault(m2,-1)  \n",
    "    self.similarity[m2].setdefault(m1,-1)  \n",
    "  \n",
    "\n",
    "    \n",
    "    i = self.userIndex[userId]\n",
    "    j = self.eventIndex[eventId]\n",
    "    js = self.userEventScores[i, :]\n",
    "    psim = self.eventPropSim[:, j]\n",
    "    csim = self.eventContSim[:, j]\n",
    "    pprod = js * psim\n",
    "    cprod = js * csim\n",
    "    pscore = 0\n",
    "    cscore = 0\n",
    "    try:\n",
    "      pscore = pprod[0, 0] - self.userEventScores[i, j]\n",
    "    except IndexError:\n",
    "      pass\n",
    "    try:\n",
    "      cscore = cprod[0, 0] - self.userEventScores[i, j]\n",
    "    except IndexError:\n",
    "      pass\n",
    "    return pscore, cscore\n",
    "\n",
    "\n",
    "\n",
    "  def rewriteData(self, start=1, train=True, header=True):\n",
    "    \"\"\"\n",
    "    把前面user-based协同过滤 和 item-based协同过滤，以及基于模型的协同过滤\n",
    "    作为特征组合在一起\n",
    "    生成新的训练数据，用于分类器分类使用\n",
    "    \"\"\"\n",
    "    fn = \"train.csv\" if train else \"test.csv\"\n",
    "    fin = open(fn, 'rb')\n",
    "    fout = open(\"data_\" + fn, 'wb')\n",
    "    # write output header\n",
    "    if header:\n",
    "      ocolnames = [\"user_reco\", \"item_reco\",\"svd_reco\"]\n",
    "      if train:\n",
    "        ocolnames.append(\"rating\")\n",
    "      fout.write(\",\".join(ocolnames) + \"\\n\")\n",
    "    \n",
    "    ln = 0\n",
    "    for line in fin:\n",
    "      ln += 1\n",
    "      if ln < start:\n",
    "        continue\n",
    "      cols = line.strip().split(\",\")\n",
    "      userId = cols[0]\n",
    "      movieId = cols[1]\n",
    "     \n",
    "      user_reco = self.userReco(userId, movieId)\n",
    "      item_reco = self.movieReco(userId, movieId)\n",
    "      svd_reco = self.svdReco(userId, movieId)\n",
    "      \n",
    "      ocols = [ user_reco, movie_reco, svd_reco]\n",
    "      if train:\n",
    "        ocols.append(self.userEventScores[userId][movieId]) # rating\n",
    "      fout.write(\",\".join(map(lambda x: str(x), ocols)) + \"\\n\")\n",
    "    fin.close()\n",
    "    fout.close()\n",
    "\n",
    "  def rewriteTrainingSet(self):\n",
    "    self.rewriteData(True)\n",
    "\n",
    "  def rewriteTestSet(self):\n",
    "    self.rewriteData(False)\n",
    "\n",
    "# When running with cython, the actual class will be converted to a .so\n",
    "# file, and the following code (along with the commented out import below)\n",
    "# will need to be put into another .py and this should be run.\n",
    "\n",
    "#import CRegressionData as rd\n",
    "\n",
    "dr = DataRewriter()\n",
    "print \"生成训练数据...\\n\"\n",
    "dr.rewriteData(train=True, start=2, header=True)\n",
    "\n",
    "print \"生成预测数据...\\n\"\n",
    "dr.rewriteData(train=False, start=2, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.建模与预测\n",
    "实际上在上述特征构造好了之后，可以用很多办法去训练得到模型和完成预测\n",
    "这里用了sklearn中的SGDClassifier\n",
    "事实上xgboost有更好的效果\n",
    "\n",
    "注意交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qing/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/qing/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 建模与预测\n",
    "from __future__ import division\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def train():\n",
    "  \"\"\"\n",
    "  在我们得到的特征上训练分类器，target为1(感兴趣)，或者是0(不感兴趣)\n",
    "  \"\"\"\n",
    "  trainDf = pd.read_csv(\"data_train.csv\")\n",
    "  X = np.matrix(pd.DataFrame(trainDf, index=None,\n",
    "    columns=[\"invited\", \"user_reco\", \"evt_p_reco\", \"evt_c_reco\",\n",
    "    \"user_pop\", \"frnd_infl\", \"evt_pop\"]))\n",
    "  y = np.array(trainDf.interested)\n",
    "    \n",
    "  clf = SGDClassifier(loss=\"log\", penalty=\"l2\")\n",
    "  clf.fit(X, y)\n",
    "  return clf\n",
    "\n",
    "def validate():\n",
    "  \"\"\"\n",
    "  10折的交叉验证，并输出交叉验证的平均准确率\n",
    "  \"\"\"\n",
    "  trainDf = pd.read_csv(\"data_train.csv\")\n",
    "  X = np.matrix(pd.DataFrame(trainDf, index=None,\n",
    "    columns=[\"invited\", \"user_reco\", \"evt_p_reco\", \"evt_c_reco\",\n",
    "    \"user_pop\", \"frnd_infl\", \"evt_pop\"]))\n",
    "  y = np.array(trainDf.interested)\n",
    "    \n",
    "  nrows = len(trainDf)\n",
    "  kfold = KFold(nrows, 10)\n",
    "  avgAccuracy = 0\n",
    "  run = 0\n",
    "  for train, test in kfold:\n",
    "    Xtrain, Xtest, ytrain, ytest = X[train], X[test], y[train], y[test]\n",
    "    clf = SGDClassifier(loss=\"log\", penalty=\"l2\")\n",
    "    clf.fit(Xtrain, ytrain)\n",
    "    \n",
    "    accuracy = 0\n",
    "    ntest = len(ytest)\n",
    "    for i in range(0, ntest):\n",
    "      yt = clf.predict(Xtest[i, :])\n",
    "      if yt == ytest[i]:\n",
    "        accuracy += 1\n",
    "    accuracy = accuracy / ntest\n",
    "    \n",
    "    print \"accuracy (run %d): %f\" % (run, accuracy)\n",
    "    avgAccuracy += accuracy\n",
    "    run += 1\n",
    "  print \"Average accuracy\", (avgAccuracy / run)\n",
    "\n",
    "def test(clf):\n",
    "  \"\"\"\n",
    "  读取test数据，用分类器完成预测\n",
    "  \"\"\"\n",
    "  origTestDf = pd.read_csv(\"test.csv\")\n",
    "  users = origTestDf.user\n",
    "  events = origTestDf.event\n",
    "    \n",
    "  testDf = pd.read_csv(\"data_test.csv\")\n",
    "  fout = open(\"result.csv\", 'wb')\n",
    "  fout.write(\",\".join([\"user\", \"event\", \"outcome\", \"dist\"]) + \"\\n\")\n",
    "  nrows = len(testDf)\n",
    "  Xp = np.matrix(testDf)\n",
    "  yp = np.zeros((nrows, 2))\n",
    "  for i in range(0, nrows):\n",
    "    xp = Xp[i, :]\n",
    "    yp[i, 0] = clf.predict(xp)\n",
    "    yp[i, 1] = clf.decision_function(xp)\n",
    "    fout.write(\",\".join(map(lambda x: str(x), \n",
    "      [users[i], events[i], yp[i, 0], yp[i, 1]])) + \"\\n\")\n",
    "  fout.close()\n",
    "\n",
    "\n",
    "clf = train()\n",
    "test(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.生成要提交的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 处理成提交结果的格式\n",
    "from __future__ import division\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def byDist(x, y):\n",
    "  return int(y[1] - x[1])\n",
    "\n",
    "def generate_submition_file():\n",
    "  # 输出文件\n",
    "  fout = open(\"final_result.csv\", 'wb')\n",
    "  fout.write(\",\".join([\"User\", \"Events\"]) + \"\\n\")\n",
    "  resultDf = pd.read_csv(\"result.csv\")\n",
    "  # group remaining user/events\n",
    "  grouped = resultDf.groupby(\"user\")\n",
    "  for name, group in grouped:\n",
    "    user = str(name)\n",
    "    tuples = zip(list(group.event), list(group.dist), list(group.outcome))\n",
    "#    tuples = filter(lambda x: x[2]==1, tuples)\n",
    "    tuples = sorted(tuples, cmp=byDist)\n",
    "    events = \"\\\"\" + str(map(lambda x: x[0], tuples)) + \"\\\"\"\n",
    "    fout.write(\",\".join([user, events]) + \"\\n\")\n",
    "  fout.close()\n",
    "\n",
    "\n",
    "generate_submition_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
